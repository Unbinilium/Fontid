{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "```\n",
    "aliyun-python-sdk-core==2.13.25\n",
    "aliyun-python-sdk-ocr==1.0.8\n",
    "Flask==1.1.2\n",
    "imutils==0.5.3\n",
    "json5==0.9.5\n",
    "Keras==2.4.3\n",
    "Keras-Preprocessing==1.1.2\n",
    "matplotlib==3.3.0\n",
    "numpy==1.18.5\n",
    "opencv-python==4.4.0.40\n",
    "oss2==2.12.1\n",
    "Pillow==7.0.0\n",
    "sklearn==0.0\n",
    "tensorflow==2.3.0\n",
    "trdg==1.6.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Aliyun python SDK modules\n",
    "\n",
    "- `aliyun-python-sdk-core`\n",
    "- `aliyun-python-sdk-ocr`\n",
    "- `oss2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aliyun SDK Core\n",
    "from aliyunsdkcore.client import AcsClient\n",
    "from aliyunsdkcore.acs_exception.exceptions import ClientException\n",
    "from aliyunsdkcore.acs_exception.exceptions import ServerException\n",
    "\n",
    "#Aliyun SDK OSS\n",
    "import oss2\n",
    "\n",
    "#Aliyun SDK OCR\n",
    "from aliyunsdkocr.request.v20191230.RecognizeCharacterRequest import RecognizeCharacterRequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Aliyun python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access_key\n",
    "access_key_id = ''\n",
    "access_key_secret = ''\n",
    "\n",
    "#OSS\n",
    "endpoint = ''\n",
    "bucket_name = ''\n",
    "\n",
    "auth = oss2.Auth(access_key_id, access_key_secret)\n",
    "bucket = oss2.Bucket(auth, endpoint, bucket_name)\n",
    "\n",
    "#OCR\n",
    "location = ''\n",
    "\n",
    "client = AcsClient(access_key_id, access_key_secret, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct OCR request body, set return format to `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = RecognizeCharacterRequest()\n",
    "request.set_accept_format('json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload local image with SHA1 hashed name to OSS\n",
    "\n",
    "- `image_path` is pointed to the local image\n",
    "- image format should be `.png`\n",
    "- image size should less than 3MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hashlib import sha1\n",
    "\n",
    "image_path = ''\n",
    "\n",
    "#Upload with SHA1 hashed name\n",
    "filename, file_extension = os.path.splitext(image_path)\n",
    "key = sha1(open(image_path, 'rb').read()).hexdigest() + file_extension\n",
    "bucket.put_object_from_file(key, image_path)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get image info from OSS\n",
    "info = bucket.get_object(key, process = 'image/info')\n",
    "info_content = info.read()\n",
    "decoded_info = json.loads(oss2.to_unicode(info_content))\n",
    "\n",
    "print('Image Info ->')\n",
    "print(json.dumps(decoded_info, indent = 4, sort_keys = True))\n",
    "\n",
    "#Struct image URL\n",
    "image_url = 'https://' + bucket_name + '.' + endpoint.replace(\"https://\",\"\") + '/' + key\n",
    "\n",
    "print('Image URL -> ' + image_url)\n",
    "\n",
    "#Set OCR image_url\n",
    "request.set_ImageURL(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send request and show OCR result\n",
    "\n",
    "- `MinHeight` is set to $\\frac{1}{20}$ of the image width\n",
    "- `OutputProbability` is set to `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-config request\n",
    "min_height = int(int(decoded_info['ImageHeight']['value']) / 20)\n",
    "request.set_MinHeight(int(min_height))\n",
    "request.set_OutputProbability(True)\n",
    "\n",
    "#Send request to OCR server and get response\n",
    "response = client.do_action_with_exception(request)\n",
    "\n",
    "#Delete OSS image\n",
    "bucket.delete_object(key)\n",
    "\n",
    "import json\n",
    "\n",
    "#Parse json response\n",
    "parsed = json.loads(response)\n",
    "\n",
    "print('Response ->')\n",
    "print(json.dumps(parsed, indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsed all `TextRectangle` and calculate the distance between image center and rect center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "objects = parsed['Data']['Results']\n",
    "\n",
    "#Cal image center O(o_x0, o_y0)\n",
    "o_x0, o_y0 = int(decoded_info['ImageWidth']['value']) / 2.0, int(decoded_info['ImageHeight']['value']) / 2.0\n",
    "\n",
    "import math\n",
    "\n",
    "for object in objects:\n",
    "    \n",
    "    #Cal TextRectangle angle A, start point A(x0, y0) and endpoint B(x1, y1)\n",
    "    A = object['TextRectangles']['Angle'] / 180.0\n",
    "    x0, y0 = object['TextRectangles']['Left'], object['TextRectangles']['Top']\n",
    "    x1, y1 = x0 + object['TextRectangles']['Width'], y0 + object['TextRectangles']['Height']\n",
    "    \n",
    "    #Cal vector AB = (v_x0, v_y0)\n",
    "    v_x0, v_y0 = x1 - x0, y1 - y0\n",
    "    \n",
    "    #Cal angle A rotated and 1/2 lenthed vector AB' = (v_x1, v_y1)\n",
    "    v_x1, v_y1 = (v_x0 * math.cos(A) - v_y0 * math.sin(A)) / 2.0, (v_y0 * math.cos(A) + v_x0 * math.sin(A)) / 2.0\n",
    "    \n",
    "    #Cal TextRectangle center point B'(x2, y2)\n",
    "    x2, y2 = x0 + v_x1, y0 + v_y1\n",
    "    \n",
    "    print('TextRectangleCtr -> ', (x2, y2))\n",
    "    \n",
    "    #Cal distance between point B and O\n",
    "    d = math.pow(x2 - o_x0, 2) + math.pow(y2 - o_y0, 2)\n",
    "    distances.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the nearest `TextRectangle` index to the image center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = distances.index(min(distances))\n",
    "\n",
    "print('Min_Index -> ', index_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw all `TextRectangle`\n",
    "\n",
    "- ROI is **green** and others is **red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imread(image_path)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "index = 0\n",
    "\n",
    "for object in objects:\n",
    "    if (index == index_min):\n",
    "        c = 'g'\n",
    "    else:\n",
    "        c = 'r'\n",
    "        \n",
    "    index += 1\n",
    "        \n",
    "    ret = patches.Rectangle(\n",
    "        (object['TextRectangles']['Left'], object['TextRectangles']['Top']),\n",
    "        object['TextRectangles']['Width'],\n",
    "        object['TextRectangles']['Height'],\n",
    "        object['TextRectangles']['Angle'] / 180.0,\n",
    "        linewidth = 2,\n",
    "        edgecolor = c,\n",
    "        facecolor = 'none'\n",
    "    )\n",
    "    \n",
    "    ax.add_patch(ret)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "A = - objects[index_min]['TextRectangles']['Angle'] / 180.0\n",
    "\n",
    "roi = PIL.Image.open(image_path)\n",
    "roi = roi.rotate(A)\n",
    "\n",
    "def rotate(x, y, o_x, o_y, theta):\n",
    "    x_r = math.cos(theta) * (x - o_x) - math.sin(theta) * (y - o_y) + o_x\n",
    "    y_r = math.sin(theta) * (x - o_x) + math.cos(theta) * (y - o_y) + o_y\n",
    "    return [x_r, y_r]\n",
    "\n",
    "#Cal start point A(x0, y0)\n",
    "x0, y0 = objects[index_min]['TextRectangles']['Left'], objects[index_min]['TextRectangles']['Top']\n",
    "\n",
    "#Cal angle A rotated A'(x1, y1)\n",
    "x1, y1 = rotate(x0, y0, o_x0, o_y0, A)\n",
    "\n",
    "roi = roi.crop((x1, y1, (x1 + objects[index_min]['TextRectangles']['Width']), (y1 + objects[index_min]['TextRectangles']['Height'])))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(roi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image function for DeepFont\n",
    "\n",
    "- color to gray\n",
    "- resize to (105, 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "def pil_image(img_path):\n",
    "    pil_img = PIL.Image.open(img_path).convert('L')\n",
    "    pil_img = pil_img.resize((105, 105))\n",
    "    return pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function\n",
    "\n",
    "- Noise a small Gaussian noise with 0 mean and standard deviation 3 is added to input.\n",
    "- Blur a random Gaussian blur with standard deviation from 2.5 to 3.5 is added to input.\n",
    "- Perspective Rotation a randomly-parameterized affine transformation is added to input.\n",
    "- Shading the input background is filled with a gradient in illumination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def noise_image(img):\n",
    "    img_array = np.asarray(img)\n",
    "    mean = 0.0\n",
    "    std = 3\n",
    "    noisy_img = img_array + np.random.normal(mean, std, img_array.shape)\n",
    "    noisy_img_clipped = np.clip(noisy_img, 0, 255)\n",
    "    noise_img = PIL.Image.fromarray(np.uint8(noisy_img_clipped))\n",
    "    noise_img = noise_img.resize((105, 105))\n",
    "    return noise_img\n",
    "\n",
    "def blur_image(img):\n",
    "    blur_img = img.filter(PIL.ImageFilter.GaussianBlur(radius = 3))\n",
    "    blur_img = blur_img.resize((105, 105))\n",
    "    return blur_img\n",
    "\n",
    "def affine_rotation(img):\n",
    "    rows, columns = img.shape\n",
    "    point1 = np.float32([[10, 10], [30, 10], [10, 30]])\n",
    "    point2 = np.float32([[20, 15], [40, 10], [20, 40]])\n",
    "    anchor = cv2.getAffineTransform(point1, point2)\n",
    "    output = cv2.warpAffine(img, anchor, (columns, rows))\n",
    "    affine_img = PIL.Image.fromarray(np.uint8(output))\n",
    "    affine_img = affine_img.resize((105, 105))\n",
    "    return affine_img\n",
    "\n",
    "def gradient_fill(img):\n",
    "    output = cv2.Laplacian(img, cv2.CV_64F)\n",
    "    laplacian_img = PIL.Image.fromarray(np.uint8(output))\n",
    "    laplacian_img = laplacian_img.resize((105, 105))\n",
    "    return laplacian_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Datasets\n",
    "\n",
    "- `ttf_path` is a folder contains all the font file with correct font name and `.ttf` extension\n",
    "- `data_path` is a folder stores or contains generated datasets\n",
    "\n",
    "Uses `TextRecognitionDataGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ttf_path = ''\n",
    "data_path = ''\n",
    "\n",
    "for file in os.listdir(ttf_path):\n",
    "    if file.endswith('.ttf'):\n",
    "        path = os.path.join(ttf_path, file)\n",
    "        name, ext = os.path.splitext(os.path.basename(path))\n",
    "        out_path = data_path + '/' + name\n",
    "        command = 'trdg -l en -c 30 -rs -let -num -r --length 1 -b 1 -e .png -fi -f 105 -ft ' + path + ' --output_dir ' + out_path \n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datasets\n",
    "\n",
    "- `label_path` should be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from imutils import paths\n",
    "from random import seed, shuffle\n",
    "\n",
    "label_path = ''\n",
    "\n",
    "#Random image path from data_path\n",
    "image_paths = sorted(list(paths.list_images(data_path)))\n",
    "seed(10)\n",
    "shuffle(image_paths)\n",
    "\n",
    "#Use folder name in data_path as font name\n",
    "font_names = []\n",
    "\n",
    "for f in os.listdir(data_path):\n",
    "    if not f.startswith('.'):\n",
    "        font_names.append(f)\n",
    "        \n",
    "font_names.sort()\n",
    "\n",
    "with open(label_path, 'w') as outfile:\n",
    "    json.dump(font_names, outfile)\n",
    "\n",
    "print('Font Names -> ', font_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling font by the index of font name in `font_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_label(label):\n",
    "    return font_names.index(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "auguments = [\"blur\", \"noise\", \"affine\", \"gradient\"]\n",
    "\n",
    "for path in image_paths:\n",
    "    \n",
    "    #Labeling images\n",
    "    label = path.split(os.path.sep)[-2]\n",
    "    \n",
    "    if not label.startswith('.'):\n",
    "        label = conv_label(label)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    pil_img = pil_image(path)\n",
    "    org_img = img_to_array(pil_img)\n",
    "    \n",
    "    data.append(org_img)\n",
    "    labels.append(label)\n",
    "    \n",
    "    #Random auguments combinations\n",
    "    for i in range(0, len(auguments)):\n",
    "        for augument in list(itertools.combinations(auguments, i + 1)):\n",
    "            \n",
    "            temp_img = pil_img\n",
    "            combinations = list(augument)\n",
    "            \n",
    "            for method in combinations:\n",
    "                if method == 'noise':\n",
    "                    temp_img = noise_image(temp_img)\n",
    "                    \n",
    "                elif method == 'blur':\n",
    "                    temp_img = blur_image(temp_img)\n",
    "                    \n",
    "                elif method == 'affine':\n",
    "                    open_cv_affine = np.array(pil_img)\n",
    "                    temp_img = affine_rotation(open_cv_affine)\n",
    "\n",
    "                elif method == 'gradient':\n",
    "                    open_cv_gradient = np.array(pil_img)\n",
    "                    temp_img = gradient_fill(open_cv_gradient)\n",
    "  \n",
    "            temp_img = img_to_array(temp_img)\n",
    "    \n",
    "            data.append(temp_img)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Datasets and transform\n",
    "\n",
    "- $\\frac{3}{4}$ for training\n",
    "- $\\frac{1}{4}$ for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Partition\n",
    "data = np.asarray(data, dtype = \"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size = 0.25, random_state = 10)\n",
    "\n",
    "#Converting labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes = len(font_names))\n",
    "testY = to_categorical(testY, num_classes = len(font_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Datasets process\n",
    "\n",
    "- **Variable Character Spacing** when rendering each synthetic image, set the character spacing (by pixel) to be a Gaussian random variable of mean 10 and standard deviation 40, bounded by [0, 50].\n",
    "- **Variable Aspect Ratio** Before cropping each image into a input patch, the image, with heigh fixed, is squeezed in width by a random ratio, drawn from a uniform distribution between $\\frac{5}{6}$ and $\\frac{7}{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "augmented_images = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-arrange Datasets channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "- **Unsupervised cross-domain sub-network ${C_u}$**, which consists of the first *K* layers of *CNN*. It accounts for extracting low-level visual features shared by both syn- thetic and real-world data domains. ${C_u}$ will be trained in a unsupervised way, using unlabeled data from both domains. It constitutes the crucial step that further minimizes the low-level feature gap, beyond the previous data augmentation efforts.\n",
    "\n",
    "- **Supervised domain-specific sub-network ${C_s}$**, which consists of the remaining *N − K* layers. It accounts for learning higher-level discriminative features for classi- fication, based on the shared features from ${C_u}$. ${C_s}$ will be trained in a supervised way, using labeled data from the synthetic domain only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D , UpSampling2D ,Conv2DTranspose\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    #Cu Layers \n",
    "    model.add(Conv2D(64, kernel_size = (48, 48), activation = 'relu', input_shape = (105, 105, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = (24, 24), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    \n",
    "    model.add(Conv2DTranspose(128, (24, 24), strides = (2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'uniform'))\n",
    "    model.add(UpSampling2D(size = (2, 2)))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, (12, 12), strides = (2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'uniform'))\n",
    "    model.add(UpSampling2D(size = (2, 2)))\n",
    "\n",
    "    #Cs Layers\n",
    "    model.add(Conv2D(256, kernel_size=(12, 12), activation = 'relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(12, 12), activation = 'relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(12, 12), activation = 'relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2383, activation = 'relu'))\n",
    "    model.add(Dense(len(font_names), activation = 'softmax'))\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "model= create_model()\n",
    "opt = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'mean_squared_error', optimizer = opt, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and store Model\n",
    "\n",
    "- `model_path` should be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "my_callbacks = [\n",
    "    callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 10, verbose = 0, mode = 'min'),\n",
    "    callbacks.ModelCheckpoint(model_path, monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    verbose = 1,\n",
    "    validation_data = (testX, testY),\n",
    "    callbacks = my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "model = load_model(model_path)\n",
    "score = model.evaluate(testX, testY, verbose = 0)\n",
    "\n",
    "print('Test loss ->', score[0])\n",
    "print('Test accuracy ->', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revert font name from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_conv_label(label):\n",
    "    return font_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pylab as plt\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "#Load image and de-noisy\n",
    "tmp_img = roi.copy().convert('L')\n",
    "tmp_img = blur_image(tmp_img)\n",
    "arr_img = img_to_array(tmp_img)\n",
    "\n",
    "#Predict using trained model\n",
    "data = []\n",
    "data.append(arr_img)\n",
    "data = np.asarray(data, dtype = \"float\") / 255.0\n",
    "y = np.argmax(model.predict(data), axis = -1)\n",
    "\n",
    "#Display result\n",
    "label = rev_conv_label(int(y[0]))\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(roi, interpolation = 'nearest', cmap = cm.gray)\n",
    "ax.text(5, 5, label, bbox = {'facecolor': 'white', 'pad': 8})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
